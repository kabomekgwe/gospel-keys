# AI Integration Guide
## Multi-Model LLM Strategy for Cost-Free Content Generation

> **Last Updated**: December 2024
> **Status**: Phi-3.5 Mini integrated ‚úÖ | Qwen2.5-7B planned üîÑ

---

## Overview

Gospel Keys implements a **3-tier local-first AI strategy** optimized for Apple Silicon (M4 Pro) that handles 90%+ of AI tasks locally, eliminating API costs while maintaining high-quality content generation for curriculum, tutorials, music theory, and exercise content.

### Cost Comparison

| Approach | Monthly Cost | Annual Cost | Scalability |
|----------|--------------|-------------|-------------|
| **Cloud Only** (Gemini) | $13-28 | $156-336 | $$$$ per user |
| **Current** (Hybrid) | $13-28 | $156-336 | High API usage |
| **Multi-Model** (Proposed) | $2-5 | $24-60 | **Zero per user** |

**Savings**: $120-276/year (conservative) | $1,320-2,760/year at scale

---

## 3-Tier Architecture

### Tier 1: Small Fast Model ‚úÖ ACTIVE

**Model**: Phi-3.5 Mini (3.8B params, 4-bit quantized)

**Specifications**:
- **Parameters**: 3.8 billion
- **Quantization**: 4-bit
- **Memory**: 2-3GB RAM
- **Speed**: ~50 tokens/sec on M4 Pro
- **Context**: 8K tokens
- **Download**: 2.3GB (one-time)

**Use Cases** (Complexity 1-4):
- ‚úÖ Exercise content generation
- ‚úÖ Chord progression generation
- ‚úÖ Content validation
- ‚úÖ Simple theory lookups
- ‚úÖ Practice tips

**Status**: Integrated via `local_llm_service.py`

**Performance**:
```python
# Example: Generate exercise content
response = local_llm.generate(
    prompt="Generate a gospel piano exercise in C major",
    max_tokens=512,
    temperature=0.7
)
# Latency: 50-150ms (vs 1-2s Gemini)
```

---

### Tier 2: Medium Quality Model üîÑ PLANNED

**Model**: Qwen2.5-7B-Instruct (4-bit quantized)

**Specifications**:
- **Parameters**: 7 billion
- **Quantization**: 4-bit
- **Memory**: 4-5GB RAM
- **Speed**: ~30-40 tokens/sec on M4 Pro
- **Context**: 32K tokens
- **Download**: 4.4GB (one-time)

**Why Qwen2.5 over alternatives**:
| Model | Params | Pros | Cons |
|-------|--------|------|------|
| **Qwen2.5-7B** ‚úÖ | 7B | Best reasoning, excellent JSON output, 32K context | None - best choice |
| Mistral-7B | 7B | Good quality, popular | Qwen2.5 benchmarks better |
| Gemma 2-9B | 9B | Google's best open model | Slower, more RAM (5.4GB) |
| LLaMA 3.1-8B | 8B | Good general purpose | Qwen2.5 better at structured output |

**Use Cases** (Complexity 5-7):
- üîÑ Tutorial generation
- üîÑ Theory analysis
- üîÑ AI coaching responses
- üîÑ Assessment creation
- üîÑ Reharmonization suggestions
- üîÑ Voice leading optimization

**Expected Impact**: Eliminates ~50% of Gemini API calls

**Performance Estimate**:
```python
# Example: Generate tutorial content
response = qwen_7b.generate(
    prompt="Create a gospel piano tutorial on ii-V-I progressions",
    max_tokens=2048,
    temperature=0.7
)
# Latency: 1-3s (faster than Gemini with network)
```

---

### Tier 3: Cloud API (Strategic Use)

**Model**: Gemini 1.5 Pro

**Use Cases** (Complexity 8-10):
- Full curriculum generation from scratch
- Advanced multi-module planning
- Adaptive curriculum redesign
- Complex multi-step reasoning

**Why Keep Gemini**:
- 1M+ token context window (far exceeds local models)
- Superior reasoning for complex planning tasks
- Used rarely (~10% of workload)
- Cost minimal when used sparingly

**Monthly Volume**: ~10 complex requests = $2-5/month

---

## Task Complexity Routing

### Automatic Model Selection

```python
class AIOrchestrator:
    # Complexity thresholds
    COMPLEXITY_SMALL = 4      # 1-4: Phi-3.5 Mini (local)
    COMPLEXITY_MEDIUM = 7     # 5-7: Qwen2.5-7B (local)
    COMPLEXITY_CLOUD = 8      # 8-10: Gemini Pro (cloud)

    TASK_COMPLEXITY = {
        TaskType.CURRICULUM_PLANNING: 8,       # Gemini
        TaskType.TUTORIAL_GENERATION: 7,       # Qwen ‚Üí Gemini
        TaskType.EXERCISE_GENERATION: 4,       # Phi-3.5 ‚úÖ
        TaskType.PROGRESSION_GENERATION: 4,    # Phi-3.5 ‚úÖ
        TaskType.THEORY_ANALYSIS: 5,           # Qwen ‚Üí Gemini
        TaskType.AI_COACHING: 5,               # Qwen ‚Üí Gemini
        TaskType.CONTENT_VALIDATION: 3,        # Phi-3.5 ‚úÖ
        TaskType.ASSESSMENTS: 6,               # Qwen ‚Üí Gemini
        TaskType.REHARMONIZATIONS: 5,          # Qwen ‚Üí Gemini
    }
```

### Content Type Distribution

| Complexity | Tasks | Current Model | Target Model | Volume |
|------------|-------|---------------|--------------|--------|
| 1-4 | Exercise, Progression, Validation | Phi-3.5 ‚úÖ | Phi-3.5 ‚úÖ | 40% |
| 5-7 | Tutorial, Theory, Coaching, Assessments | Gemini | Qwen2.5 üîÑ | 50% |
| 8-10 | Curriculum Planning | Gemini | Gemini | 10% |

**Result**: 90% local (vs 40% today)

---

## AI-Generated Content Types

### 1. Curriculum Generation

**Current**: Gemini Pro (complexity 8)
**Target**: Gemini Pro (keep)

**What's Generated**:
```json
{
  "title": "Gospel Piano Mastery",
  "duration_weeks": 12,
  "modules": [
    {
      "title": "Gospel Fundamentals",
      "week_start": 1,
      "week_end": 4,
      "lessons": [
        {
          "title": "Extended Voicings in Gospel",
          "concepts": ["maj9", "7#9", "13th chords"],
          "exercises": [...]
        }
      ]
    }
  ]
}
```

**Features**:
- Personalized to skill level
- Multi-week structured plans
- Adaptive difficulty

**Storage**: Database (`curriculum_models.py`)

---

### 2. Tutorial Generation

**Current**: Gemini Pro (complexity 7)
**Target**: Qwen2.5-7B (complexity 7)

**What's Generated**:
```json
{
  "overview": {
    "what_you_will_learn": ["Extended voicings", "Voice leading"],
    "duration_minutes": 60,
    "difficulty": "intermediate"
  },
  "theory": {
    "summary": "2-3 paragraph explanation...",
    "key_points": ["Point 1", "Point 2"],
    "examples": ["Example with explanation"]
  },
  "practice_guide": {
    "steps": [
      {
        "step": 1,
        "title": "Step title",
        "instruction": "Detailed instruction",
        "success_criteria": "How to know you've mastered"
      }
    ]
  },
  "common_mistakes": [...],
  "next_steps": {...}
}
```

**Storage**: `CurriculumLesson.tutorial_content_json`
**Cache**: 30 days (720 hours)

---

### 3. Exercise Generation

**Current**: Phi-3.5 Mini ‚úÖ (complexity 4)
**Target**: Phi-3.5 Mini ‚úÖ (keep)

**What's Generated**:
- Chord progressions (gospel, jazz, blues styles)
- Scale patterns (major, minor, modes)
- Voicing studies (drop-2, rootless, shell)
- Jazz licks (bebop, blues, gospel)
- Rhythm exercises

**Example**:
```python
{
  "type": "progression",
  "content": {
    "chords": ["Dm7", "G7", "Cmaj7"],
    "key": "C",
    "roman_numerals": ["ii7", "V7", "Imaj7"]
  },
  "difficulty": "intermediate"
}
```

**Storage**: `CurriculumExercise.content_json`

---

### 4. Theory Analysis

**Current**: Gemini Flash (complexity 5)
**Target**: Qwen2.5-7B (complexity 5)

**What's Generated**:
- Chord function analysis
- Scale-chord relationships
- Voice leading explanations
- Harmonic progressions
- Substitution suggestions

**Example Use Case**:
```python
# Analyze chord progression
result = ai.analyze_theory(
    chords=["Cmaj7", "A7", "Dm7", "G7"],
    key="C major"
)
# Returns: Theory explanation + Roman numerals + function
```

---

### 5. AI Coaching

**Current**: Gemini Flash (complexity 5)
**Target**: Qwen2.5-7B (complexity 5)

**What it does**:
- Real-time conversational coaching
- Context-aware (knows user's skill level and progress)
- Performance-aware guidance
- Encouraging tone

**Example**:
```python
# User asks: "How do I practice ii-V-I progressions?"
coach_response = ai_coach.respond(
    user_message="How do I practice ii-V-I progressions?",
    user_context={
        "skill_level": 6,
        "recent_performance": {"progressions": 0.75}
    }
)
# Returns: Personalized practice advice
```

---

### 6. Assessments

**Current**: Gemini Flash (complexity 6)
**Target**: Qwen2.5-7B (complexity 6)

**What's Generated**:
- Diagnostic tests (initial skill evaluation)
- Milestone evaluations (progress checks)
- Module assessments (end-of-module tests)
- Final exams (curriculum completion)

**Example**:
```json
{
  "type": "milestone",
  "questions": [
    {
      "question": "Play a rootless Dm7 voicing",
      "type": "performance",
      "scoring_criteria": {...}
    }
  ],
  "feedback": "Personalized feedback based on results",
  "recommendations": ["Focus on voice leading"]
}
```

---

## Model Integration Architecture

### File Structure

```
backend/app/
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ ai_orchestrator.py         # Task routing logic
‚îÇ   ‚îú‚îÄ‚îÄ local_llm_service.py       # Phi-3.5 Mini (current)
‚îÇ   ‚îú‚îÄ‚îÄ multi_model_service.py     # Multi-model manager (planned)
‚îÇ   ‚îú‚îÄ‚îÄ ai_generator.py            # Music-specific generators
‚îÇ   ‚îú‚îÄ‚îÄ tutorial_service.py        # Tutorial generation
‚îÇ   ‚îú‚îÄ‚îÄ curriculum_service.py      # Curriculum management
‚îÇ   ‚îú‚îÄ‚îÄ assessment_service.py      # Assessment creation
‚îÇ   ‚îî‚îÄ‚îÄ adaptive_curriculum_service.py  # Adaptive learning
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ ... (future: local model wrappers)
‚îÇ
‚îî‚îÄ‚îÄ database/
    ‚îî‚îÄ‚îÄ curriculum_models.py       # Data schemas
```

### Current Implementation (Phi-3.5 Mini)

**Location**: `/backend/app/services/local_llm_service.py`

```python
class LocalLLMService:
    """Local LLM using MLX on M4 Neural Engine"""

    def __init__(self):
        self.model_name = "mlx-community/Phi-3.5-mini-instruct-4bit"
        self.model, self.tokenizer = load(self.model_name)

    def generate(self, prompt: str, max_tokens: int = 512) -> str:
        """Generate text using M4 Neural Engine"""
        formatted_prompt = f"<|user|>\n{prompt}<|end|>\n<|assistant|>\n"
        response = generate(
            model=self.model,
            tokenizer=self.tokenizer,
            prompt=formatted_prompt,
            max_tokens=max_tokens
        )
        return response.strip()
```

**Usage**:
```python
from app.services.local_llm_service import local_llm_service

# Generate content
result = local_llm_service.generate(
    prompt="Generate a gospel piano exercise in C major",
    max_tokens=512
)
```

---

## Setup Instructions

### Prerequisites

- **Hardware**: Apple Silicon Mac (M1/M2/M3/M4)
- **RAM**: 8GB minimum, 16GB+ recommended
- **Disk**: 10GB free space
- **Python**: 3.10 or 3.11

### Install MLX Framework

```bash
# Install MLX and MLX-LM
pip install mlx>=0.30.0
pip install mlx-lm>=0.28.4
```

### Download Models

**Phi-3.5 Mini** (already integrated):
```python
# First run will auto-download (2.3GB)
from mlx_lm import load
model, tokenizer = load("mlx-community/Phi-3.5-mini-instruct-4bit")
```

**Qwen2.5-7B** (planned):
```python
# When ready to integrate
from mlx_lm import load
model, tokenizer = load("mlx-community/Qwen2.5-7B-Instruct-4bit")
# Downloads 4.4GB (one-time)
```

### Enable Local LLM

**Environment Variable**:
```bash
# .env
FORCE_LOCAL_LLM=false  # Default: use routing (Gemini for complex)
FORCE_LOCAL_LLM=true   # Force all tasks to local (testing)
```

**Runtime Check**:
```python
# Check if local LLM is available
from app.services.ai_orchestrator import ai_orchestrator

status = ai_orchestrator.get_status()
print(status)
# {
#   "local_llm_available": True,
#   "gemini_available": True,
#   "budget_mode": "balanced"
# }
```

---

## Performance Benchmarks

### Token Generation Speed (M4 Pro)

| Model | Params | Quantization | Tokens/Sec | Latency (512 tokens) |
|-------|--------|--------------|------------|---------------------|
| Phi-3.5 Mini | 3.8B | 4-bit | ~50 | ~10s |
| Qwen2.5-7B | 7B | 4-bit | ~35 | ~14s |
| Gemini Flash | Large | Cloud | ~40 | ~12s + network |
| Gemini Pro | Huge | Cloud | ~30 | ~17s + network |

**Key Insight**: Local models are competitive or faster when network latency is factored in!

### Quality Comparison

| Task | Phi-3.5 Mini | Qwen2.5-7B | Gemini Pro |
|------|--------------|------------|------------|
| Tutorial generation | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Theory analysis | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Exercise generation | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| JSON output | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Curriculum planning | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

### Memory Usage

| Scenario | Phi-3.5 Only | Phi-3.5 + Qwen2.5 | M4 Pro Capacity |
|----------|--------------|-------------------|-----------------|
| Inference | 2-3GB | 6-8GB (both loaded) | 24GB |
| Training | N/A | 12-16GB | 24GB |
| **Headroom** | 21GB | 16GB | - |

**Conclusion**: Plenty of RAM to run both models simultaneously on M4 Pro!

---

## Cost Analysis

### Current State (Gemini Only)

**Monthly Usage Estimate**:
- Curriculum generation: 10 requests √ó $1.25/1M tokens = $2-5
- Tutorial generation: 50 requests √ó $1.25/1M tokens = $10-20
- Theory/coaching: 200 requests √ó $0.075/1M tokens = $1-3
- Exercises: 500 requests √ó $0 (local) = $0

**Total**: $13-28/month

### After Multi-Model Integration

**Monthly Usage Estimate**:
- Curriculum generation: 10 requests √ó $1.25/1M tokens = $2-5
- Tutorial generation: 50 requests √ó $0 (Qwen local) = $0
- Theory/coaching: 200 requests √ó $0 (Qwen local) = $0
- Exercises: 500 requests √ó $0 (Phi-3.5 local) = $0

**Total**: $2-5/month

**Savings**: $10-23/month = **$120-276/year**

### At Scale (10x Users)

**Current**: $130-280/month
**After**: $20-50/month
**Savings**: ~$110-230/month = **$1,320-2,760/year** üí∞

---

## Migration Roadmap

### Phase 1: Add Qwen2.5-7B (Week 1)

**Tasks**:
1. Create `multi_model_service.py`
2. Implement model switching logic
3. Update `ai_orchestrator.py` routing
4. Test tutorial/theory generation quality

**Success Metrics**:
- Tutorial quality: 90%+ vs Gemini
- Theory accuracy: 85%+
- 50%+ reduction in Gemini calls

### Phase 2: Optimize & Cache (Week 2)

**Tasks**:
1. Keep both models loaded (no reload penalty)
2. Enhanced caching (7-day tutorials, 30-day theory)
3. Batch generation scripts

**Success Metrics**:
- Model switch time: <1s
- Cache hit rate: >40%
- Batch speed: 10x improvement

### Phase 3: Pre-Generate Library (Week 3)

**Tasks**:
1. Pre-generate 100+ tutorials
2. Pre-generate theory content
3. Pre-generate 500+ exercise templates

**Success Metrics**:
- 1000+ pre-generated items
- Zero AI calls for common content
- 100% offline capable

---

## Monitoring & Metrics

### Key Metrics to Track

1. **Cost Metrics**:
   - Monthly Gemini API spend
   - Requests by model (Phi-3.5, Qwen, Gemini)
   - Cost per content type

2. **Performance Metrics**:
   - Average generation time by model
   - Model switching overhead
   - Cache hit rates

3. **Quality Metrics**:
   - User ratings (tutorial coherence)
   - JSON parse success rate
   - Theory accuracy (expert validation)

### Logging

```python
# Track model usage
logger.info(f"Generated content using {model_name}")
logger.info(f"Latency: {latency_ms}ms")
logger.info(f"Tokens: {token_count}")
logger.info(f"Cost: ${cost}")
```

---

## Troubleshooting

### Issue: Local LLM Not Loading

**Symptoms**: `LOCAL_LLM_ENABLED = False`

**Solutions**:
1. Check MLX installation: `python -c "import mlx.core; print(mlx.core.default_device())"`
2. Verify model download: `ls ~/.cache/huggingface/hub/`
3. Check Python version: Must be 3.10 or 3.11
4. Restart FastAPI server

### Issue: Out of Memory

**Symptoms**: Process killed during generation

**Solutions**:
1. Reduce batch size
2. Use 4-bit quantization (already default)
3. Don't load both models simultaneously (use lazy loading)
4. Close other apps

### Issue: Slow Generation

**Symptoms**: >5s for simple tasks

**Solutions**:
1. Verify running on M4 (not in Rosetta)
2. Check CPU/GPU usage
3. Reduce max_tokens
4. Use faster model (Phi-3.5 instead of Qwen)

---

## Future Enhancements

### Planned Features

1. **Fine-Tuning** (Phase 4):
   - Fine-tune Qwen2.5-7B on gospel corpus
   - LoRA adapters for each genre
   - Better quality than base model

2. **Curriculum Templates**:
   - Pre-built templates (no AI needed)
   - One-click activation
   - Customizable with local LLM

3. **Hybrid Generation**:
   - Gemini for outline, Qwen for details
   - Best of both worlds

4. **Adaptive Routing**:
   - Track quality scores
   - Auto-adjust complexity thresholds
   - Learn optimal routing over time

---

## References

- **MLX Framework**: https://github.com/ml-explore/mlx
- **MLX LM**: https://github.com/ml-explore/mlx-lm
- **Phi-3.5 Mini**: https://huggingface.co/microsoft/Phi-3.5-mini-instruct
- **Qwen2.5**: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
- **Model Hub**: https://huggingface.co/mlx-community

---

## Contact & Support

For questions about AI integration:
- Check `/backend/app/services/ai_orchestrator.py` for routing logic
- See `/backend/app/services/local_llm_service.py` for implementation
- Review this guide for setup and troubleshooting

**Last Updated**: December 2024
